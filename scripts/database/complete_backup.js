/**
 * å®Œæ•´æ•°æ®åº“å¤‡ä»½è„šæœ¬
 *
 * åŠŸèƒ½ï¼š
 * - å¤‡ä»½æ‰€æœ‰è¡¨çš„ç»“æ„ï¼ˆåŒ…å«ç´¢å¼•å’Œå¤–é”®çº¦æŸï¼‰
 * - å¤‡ä»½æ‰€æœ‰è¡¨çš„æ•°æ®ï¼ˆåŒ…æ‹¬ç©ºè¡¨ï¼‰
 * - ç”ŸæˆSQLå’ŒJSONä¸¤ç§æ ¼å¼
 * - ç”ŸæˆMD5æ ¡éªŒå’Œ
 * - ç”Ÿæˆå¤‡ä»½æ‘˜è¦æŠ¥å‘Š
 *
 * ä½¿ç”¨æ–¹å¼ï¼š
 * node scripts/database/complete_backup.js [--output-dir=å¤‡ä»½ç›®å½•]
 */

'use strict'

require('dotenv').config()
const fs = require('fs')
const path = require('path')
const crypto = require('crypto')
const { sequelize } = require('../../models')

// åŒ—äº¬æ—¶é—´è¾…åŠ©å‡½æ•°
function getBeijingTime() {
  return new Date().toLocaleString('zh-CN', {
    timeZone: 'Asia/Shanghai',
    year: 'numeric',
    month: '2-digit',
    day: '2-digit',
    hour: '2-digit',
    minute: '2-digit',
    second: '2-digit',
    hour12: false
  })
}

function getBeijingDateStr() {
  const now = new Date()
  const beijing = new Date(now.toLocaleString('en-US', { timeZone: 'Asia/Shanghai' }))
  const year = beijing.getFullYear()
  const month = String(beijing.getMonth() + 1).padStart(2, '0')
  const day = String(beijing.getDate()).padStart(2, '0')
  const hour = String(beijing.getHours()).padStart(2, '0')
  const minute = String(beijing.getMinutes()).padStart(2, '0')
  const second = String(beijing.getSeconds()).padStart(2, '0')
  return `${year}-${month}-${day}_${hour}-${minute}-${second}`
}

// é¢œè‰²è¾“å‡º
const colors = {
  reset: '\x1b[0m',
  red: '\x1b[31m',
  green: '\x1b[32m',
  yellow: '\x1b[33m',
  blue: '\x1b[34m',
  cyan: '\x1b[36m'
}

function log(message, color = 'reset') {
  console.log(`${colors[color]}${message}${colors.reset}`)
}

// MD5è®¡ç®—
function calculateMD5(filePath) {
  const content = fs.readFileSync(filePath)
  return crypto.createHash('md5').update(content).digest('hex')
}

// è½¬ä¹‰SQLå­—ç¬¦ä¸²
function escapeSQLValue(value) {
  if (value === null || value === undefined) {
    return 'NULL'
  }
  if (typeof value === 'number') {
    return value.toString()
  }
  if (typeof value === 'boolean') {
    return value ? '1' : '0'
  }
  if (value instanceof Date) {
    return `'${value.toISOString().slice(0, 19).replace('T', ' ')}'`
  }
  if (typeof value === 'object') {
    return `'${JSON.stringify(value).replace(/'/g, "''").replace(/\\/g, '\\\\')}'`
  }
  // è½¬ä¹‰å­—ç¬¦ä¸²
  return `'${String(value).replace(/'/g, "''").replace(/\\/g, '\\\\').replace(/\n/g, '\\n').replace(/\r/g, '\\r')}'`
}

async function performCompleteBackup(outputDir) {
  const startTime = Date.now()
  const beijingTime = getBeijingTime()
  const dateStr = getBeijingDateStr()

  log('\n' + '='.repeat(80), 'cyan')
  log('å®Œæ•´æ•°æ®åº“å¤‡ä»½å·¥å…· - Complete Database Backup', 'cyan')
  log('='.repeat(80), 'cyan')
  log(`\nå¤‡ä»½æ—¶é—´ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰: ${beijingTime}`, 'blue')

  const dbName = process.env.DB_NAME || 'restaurant_points_dev'
  const dbHost = process.env.DB_HOST
  const dbPort = process.env.DB_PORT

  log(`æ•°æ®åº“: ${dbName}@${dbHost}:${dbPort}\n`, 'blue')

  try {
    // éªŒè¯æ•°æ®åº“è¿æ¥
    await sequelize.authenticate()
    log('âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ\n', 'green')

    // è·å–æ‰€æœ‰è¡¨
    const [tables] = await sequelize.query('SHOW TABLES')
    const tableNames = tables.map(t => Object.values(t)[0])

    log(`ğŸ“Š å‘ç° ${tableNames.length} ä¸ªè¡¨\n`, 'blue')

    // å¤‡ä»½æ•°æ®ç»“æ„
    const backupData = {
      metadata: {
        backup_date: beijingTime,
        backup_timestamp: new Date().toISOString(),
        database: dbName,
        host: `${dbHost}:${dbPort}`,
        total_tables: tableNames.length,
        version: 'complete_backup_v2.0'
      },
      tables: {},
      indexes: {},
      foreign_keys: {},
      statistics: {
        total_rows: 0,
        empty_tables: [],
        non_empty_tables: [],
        error_tables: []
      }
    }

    // SQLå¤‡ä»½å†…å®¹
    let sqlContent = `-- å®Œæ•´æ•°æ®åº“å¤‡ä»½
-- å¤‡ä»½æ—¶é—´ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰: ${beijingTime}
-- æ•°æ®åº“: ${dbName}
-- ä¸»æœº: ${dbHost}:${dbPort}
-- è¡¨æ•°é‡: ${tableNames.length}
-- å¤‡ä»½ç‰ˆæœ¬: complete_backup_v2.0

SET NAMES utf8mb4;
SET FOREIGN_KEY_CHECKS = 0;
SET SQL_MODE = 'NO_AUTO_VALUE_ON_ZERO';

`

    // è·å–æ‰€æœ‰å¤–é”®çº¦æŸ
    const [foreignKeys] = await sequelize.query(`
      SELECT 
        TABLE_NAME,
        CONSTRAINT_NAME,
        COLUMN_NAME,
        REFERENCED_TABLE_NAME,
        REFERENCED_COLUMN_NAME
      FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
      WHERE TABLE_SCHEMA = DATABASE()
        AND REFERENCED_TABLE_NAME IS NOT NULL
      ORDER BY TABLE_NAME, CONSTRAINT_NAME
    `)

    // è·å–å¤–é”®è§„åˆ™
    const [fkRules] = await sequelize.query(`
      SELECT 
        rc.CONSTRAINT_NAME,
        rc.DELETE_RULE,
        rc.UPDATE_RULE,
        kcu.TABLE_NAME
      FROM INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS rc
      JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE kcu
        ON rc.CONSTRAINT_NAME = kcu.CONSTRAINT_NAME
        AND rc.CONSTRAINT_SCHEMA = kcu.CONSTRAINT_SCHEMA
      WHERE rc.CONSTRAINT_SCHEMA = DATABASE()
    `)

    // åˆ›å»ºå¤–é”®è§„åˆ™æ˜ å°„
    const fkRuleMap = {}
    fkRules.forEach(rule => {
      fkRuleMap[rule.CONSTRAINT_NAME] = {
        delete_rule: rule.DELETE_RULE,
        update_rule: rule.UPDATE_RULE
      }
    })

    // ç»„ç»‡å¤–é”®æ•°æ®
    foreignKeys.forEach(fk => {
      if (!backupData.foreign_keys[fk.TABLE_NAME]) {
        backupData.foreign_keys[fk.TABLE_NAME] = []
      }
      const rules = fkRuleMap[fk.CONSTRAINT_NAME] || {
        delete_rule: 'RESTRICT',
        update_rule: 'CASCADE'
      }
      backupData.foreign_keys[fk.TABLE_NAME].push({
        constraint_name: fk.CONSTRAINT_NAME,
        column: fk.COLUMN_NAME,
        references_table: fk.REFERENCED_TABLE_NAME,
        references_column: fk.REFERENCED_COLUMN_NAME,
        on_delete: rules.delete_rule,
        on_update: rules.update_rule
      })
    })

    // å¤‡ä»½æ¯ä¸ªè¡¨
    for (const tableName of tableNames) {
      log(`å¤‡ä»½è¡¨: ${tableName}...`, 'cyan')

      try {
        // 1. è·å–è¡¨ç»“æ„ï¼ˆCREATE TABLEè¯­å¥ï¼‰
        const [createTable] = await sequelize.query(`SHOW CREATE TABLE \`${tableName}\``)
        const createStatement = createTable[0]['Create Table']

        // 2. è·å–è¡¨æ•°æ®
        const [rows] = await sequelize.query(`SELECT * FROM \`${tableName}\``)
        const rowCount = rows.length

        // 3. è·å–ç´¢å¼•ä¿¡æ¯
        const [indexes] = await sequelize.query(`SHOW INDEX FROM \`${tableName}\``)

        // 4. è·å–åˆ—ä¿¡æ¯
        const [columns] = await sequelize.query(`SHOW FULL COLUMNS FROM \`${tableName}\``)

        // å­˜å‚¨åˆ°JSON
        backupData.tables[tableName] = {
          row_count: rowCount,
          create_statement: createStatement,
          columns: columns.map(col => ({
            name: col.Field,
            type: col.Type,
            null: col.Null,
            key: col.Key,
            default: col.Default,
            extra: col.Extra,
            comment: col.Comment
          })),
          data: rows
        }

        // å­˜å‚¨ç´¢å¼•
        backupData.indexes[tableName] = indexes.map(idx => ({
          key_name: idx.Key_name,
          column_name: idx.Column_name,
          non_unique: idx.Non_unique,
          seq_in_index: idx.Seq_in_index,
          index_type: idx.Index_type
        }))

        // ç”ŸæˆSQL
        sqlContent += `-- --------------------------------------------------------\n`
        sqlContent += `-- è¡¨ç»“æ„: ${tableName}\n`
        sqlContent += `-- è¡Œæ•°: ${rowCount}\n`
        sqlContent += `-- --------------------------------------------------------\n\n`
        sqlContent += `DROP TABLE IF EXISTS \`${tableName}\`;\n`
        sqlContent += `${createStatement};\n\n`

        // ç”ŸæˆINSERTè¯­å¥
        if (rowCount > 0) {
          const columnNames = columns.map(col => `\`${col.Field}\``).join(', ')

          sqlContent += `-- æ•°æ®: ${tableName}\n`
          sqlContent += `LOCK TABLES \`${tableName}\` WRITE;\n`

          // åˆ†æ‰¹æ’å…¥ï¼Œæ¯æ‰¹100æ¡
          const batchSize = 100
          for (let i = 0; i < rows.length; i += batchSize) {
            const batch = rows.slice(i, i + batchSize)
            const values = batch
              .map(row => {
                const vals = columns.map(col => escapeSQLValue(row[col.Field]))
                return `(${vals.join(', ')})`
              })
              .join(',\n')

            sqlContent += `INSERT INTO \`${tableName}\` (${columnNames}) VALUES\n${values};\n`
          }

          sqlContent += `UNLOCK TABLES;\n\n`
        }

        // æ›´æ–°ç»Ÿè®¡
        backupData.statistics.total_rows += rowCount
        if (rowCount === 0) {
          backupData.statistics.empty_tables.push(tableName)
        } else {
          backupData.statistics.non_empty_tables.push({ name: tableName, rows: rowCount })
        }

        log(`  âœ… ${tableName}: ${rowCount} è¡Œ, ${columns.length} åˆ—`, 'green')
      } catch (error) {
        log(`  âŒ ${tableName}: å¤‡ä»½å¤±è´¥ - ${error.message}`, 'red')
        backupData.statistics.error_tables.push({ name: tableName, error: error.message })
      }
    }

    // æ·»åŠ å¤–é”®çº¦æŸæ¢å¤SQL
    sqlContent += `-- --------------------------------------------------------\n`
    sqlContent += `-- å¤–é”®çº¦æŸæ¢å¤\n`
    sqlContent += `-- --------------------------------------------------------\n\n`
    sqlContent += `SET FOREIGN_KEY_CHECKS = 1;\n\n`
    sqlContent += `-- å¤‡ä»½å®Œæˆäº ${beijingTime}\n`

    // ä¿å­˜å¤‡ä»½æ–‡ä»¶
    const jsonFileName = `complete_backup_2026-01-13_${dateStr}.json`
    const sqlFileName = `complete_backup_2026-01-13_${dateStr}.sql`

    const jsonPath = path.join(outputDir, jsonFileName)
    const sqlPath = path.join(outputDir, sqlFileName)

    fs.writeFileSync(jsonPath, JSON.stringify(backupData, null, 2), 'utf8')
    fs.writeFileSync(sqlPath, sqlContent, 'utf8')

    // è®¡ç®—MD5
    const jsonMD5 = calculateMD5(jsonPath)
    const sqlMD5 = calculateMD5(sqlPath)

    // ä¿å­˜MD5æ ¡éªŒæ–‡ä»¶
    const md5Content = `# MD5 æ ¡éªŒå’Œ
# ç”Ÿæˆæ—¶é—´ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰: ${beijingTime}

${jsonFileName}: ${jsonMD5}
${sqlFileName}: ${sqlMD5}
`
    fs.writeFileSync(path.join(outputDir, 'BACKUP_MD5.txt'), md5Content)

    // ç”Ÿæˆå¤‡ä»½æ‘˜è¦
    const jsonSize = (fs.statSync(jsonPath).size / 1024 / 1024).toFixed(2)
    const sqlSize = (fs.statSync(sqlPath).size / 1024 / 1024).toFixed(2)

    const summaryContent = `# æ•°æ®åº“å¤‡ä»½æ‘˜è¦æŠ¥å‘Š
## å¤‡ä»½ä¿¡æ¯
- å¤‡ä»½æ—¥æœŸï¼ˆåŒ—äº¬æ—¶é—´ï¼‰: 2026-01-13
- å¤‡ä»½æ—¶é—´: ${beijingTime}
- æ•°æ®åº“: ${dbName}
- ä¸»æœº: ${dbHost}:${dbPort}

## å¤‡ä»½ç»Ÿè®¡
- æ€»è¡¨æ•°: ${tableNames.length}
- æ€»è¡Œæ•°: ${backupData.statistics.total_rows}
- ç©ºè¡¨æ•°: ${backupData.statistics.empty_tables.length}
- é”™è¯¯è¡¨æ•°: ${backupData.statistics.error_tables.length}

## å¤‡ä»½æ–‡ä»¶
- JSONå¤‡ä»½: ${jsonFileName} (${jsonSize} MB)
- SQLå¤‡ä»½: ${sqlFileName} (${sqlSize} MB)
- MD5æ ¡éªŒ: BACKUP_MD5.txt

## è¡¨è¯¦æƒ…
${tableNames
  .map(t => {
    const tableData = backupData.tables[t]
    const rowCount = tableData ? tableData.row_count : 0
    return `âœ… ${t}: ${rowCount} è¡Œ`
  })
  .join('\n')}

## å¤–é”®çº¦æŸç»Ÿè®¡
- æ€»å¤–é”®æ•°: ${foreignKeys.length}
- æ¶‰åŠè¡¨æ•°: ${Object.keys(backupData.foreign_keys).length}

## å¤‡ä»½å®Œæˆ
âœ… å¤‡ä»½å·²æˆåŠŸå®Œæˆäº ${beijingTime}
`
    fs.writeFileSync(path.join(outputDir, 'BACKUP_SUMMARY.txt'), summaryContent)

    // ç”ŸæˆREADME
    const readmeContent = `# æ•°æ®åº“å¤‡ä»½ - 2026å¹´01æœˆ13æ—¥ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰

## å¤‡ä»½å†…å®¹
- å®Œæ•´çš„è¡¨ç»“æ„ï¼ˆCREATE TABLEè¯­å¥ï¼‰
- æ‰€æœ‰è¡¨çš„æ•°æ®ï¼ˆåŒ…æ‹¬ç©ºè¡¨ï¼‰
- ç´¢å¼•ä¿¡æ¯
- å¤–é”®çº¦æŸåŠè§„åˆ™

## æ–‡ä»¶æ¸…å•
| æ–‡ä»¶ | è¯´æ˜ | å¤§å° |
|------|------|------|
| ${jsonFileName} | JSONæ ¼å¼å®Œæ•´å¤‡ä»½ | ${jsonSize} MB |
| ${sqlFileName} | SQLæ ¼å¼å®Œæ•´å¤‡ä»½ | ${sqlSize} MB |
| BACKUP_MD5.txt | MD5æ ¡éªŒå’Œ | - |
| BACKUP_SUMMARY.txt | å¤‡ä»½æ‘˜è¦ | - |
| README.md | è¯´æ˜æ–‡æ¡£ | - |

## æ¢å¤æ–¹æ³•

### SQLæ¢å¤
\`\`\`bash
mysql -uç”¨æˆ·å -på¯†ç  æ•°æ®åº“å < ${sqlFileName}
\`\`\`

### JSONæ¢å¤
ä½¿ç”¨é¡¹ç›®æä¾›çš„æ¢å¤è„šæœ¬æˆ–æ‰‹åŠ¨è§£æJSONæ–‡ä»¶æ¢å¤

## éªŒè¯å¤‡ä»½
\`\`\`bash
# éªŒè¯MD5
md5sum ${jsonFileName}
md5sum ${sqlFileName}
# å¯¹æ¯” BACKUP_MD5.txt ä¸­çš„å€¼
\`\`\`

## å¤‡ä»½ç»Ÿè®¡
- è¡¨æ•°é‡: ${tableNames.length}
- æ€»è¡Œæ•°: ${backupData.statistics.total_rows}
- å¤‡ä»½æ—¶é—´: ${beijingTime}

---
ç”Ÿæˆæ—¶é—´: ${beijingTime}ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰
`
    fs.writeFileSync(path.join(outputDir, 'README.md'), readmeContent)

    const duration = ((Date.now() - startTime) / 1000).toFixed(2)

    // æ‰“å°å®ŒæˆæŠ¥å‘Š
    log('\n' + '='.repeat(80), 'cyan')
    log('å¤‡ä»½å®ŒæˆæŠ¥å‘Š', 'cyan')
    log('='.repeat(80), 'cyan')

    log(`\nğŸ“Š å¤‡ä»½ç»Ÿè®¡:`, 'blue')
    log(`   è¡¨æ•°é‡: ${tableNames.length}`, 'green')
    log(`   æ€»è¡Œæ•°: ${backupData.statistics.total_rows}`, 'green')
    log(`   ç©ºè¡¨: ${backupData.statistics.empty_tables.length}ä¸ª`, 'yellow')
    log(
      `   é”™è¯¯: ${backupData.statistics.error_tables.length}ä¸ª`,
      backupData.statistics.error_tables.length > 0 ? 'red' : 'green'
    )
    log(`   å¤–é”®çº¦æŸ: ${foreignKeys.length}ä¸ª`, 'green')

    log(`\nğŸ“ å¤‡ä»½æ–‡ä»¶:`, 'blue')
    log(`   ${jsonPath} (${jsonSize} MB)`, 'green')
    log(`   ${sqlPath} (${sqlSize} MB)`, 'green')

    log(`\nâ±ï¸ è€—æ—¶: ${duration}ç§’`, 'blue')
    log(`\nâœ… æ•°æ®åº“å¤‡ä»½æˆåŠŸå®Œæˆï¼\n`, 'green')

    return {
      success: true,
      tables: tableNames.length,
      totalRows: backupData.statistics.total_rows,
      files: {
        json: jsonPath,
        sql: sqlPath
      }
    }
  } catch (error) {
    log(`\nâŒ å¤‡ä»½å¤±è´¥: ${error.message}`, 'red')
    console.error(error.stack)
    return { success: false, error: error.message }
  } finally {
    await sequelize.close()
  }
}

// ä¸»å‡½æ•°
async function main() {
  const args = process.argv.slice(2)

  // è§£æè¾“å‡ºç›®å½•
  let outputDir = path.join(process.cwd(), 'backups/backup_2026-01-13_complete')
  const outputArg = args.find(arg => arg.startsWith('--output-dir='))
  if (outputArg) {
    outputDir = outputArg.split('=')[1]
  }

  // ç¡®ä¿ç›®å½•å­˜åœ¨
  if (!fs.existsSync(outputDir)) {
    fs.mkdirSync(outputDir, { recursive: true })
  }

  const result = await performCompleteBackup(outputDir)

  process.exit(result.success ? 0 : 1)
}

if (require.main === module) {
  main()
}

module.exports = { performCompleteBackup }
